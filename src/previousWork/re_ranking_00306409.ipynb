{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# AIR - Exercise in Google Colab\n","\n","## Colab Preparation\n","\n","Open via google drive -> right click: open with Colab\n","\n","**Get a GPU**\n","\n","Toolbar -> Runtime -> Change Runtime Type -> GPU\n","\n","**Mount Google Drive**\n","\n","* Download data and clone your github repo to your Google Drive folder\n","* Use Google Drive as connection between Github and Colab (Could also use direct github access, but re-submitting credentials might be annoying)\n","* Commit to Github locally from the synced drive\n","\n","**Keep Alive**\n","\n","When training google colab tends to kick you out, This might help: https://medium.com/@shivamrawat_756/how-to-prevent-google-colab-from-disconnecting-717b88a128c0\n","\n","**Get Started**\n","\n","Run the following script to mount google drive and install needed python packages. Pytorch comes pre-installed."],"metadata":{"colab_type":"text","id":"D8usSW9Bwv4h"}},{"cell_type":"code","execution_count":null,"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install -r ../requirements.txt"],"outputs":[],"metadata":{"colab":{},"colab_type":"code","id":"Sfiw_6jZ0uWa"}},{"cell_type":"code","execution_count":null,"source":["import torch\n","\n","print(\"Version:\",torch.__version__)\n","print(\"Has GPU:\",torch.cuda.is_available()) # check that 1 gpu is available\n","print(\"Random tensor:\",torch.rand(10,device=\"cuda\")) # check that pytorch works "],"outputs":[],"metadata":{"colab":{},"colab_type":"code","id":"IUVVDw1m2sed"}},{"cell_type":"markdown","source":["# Main.py Replacement\n","\n","-> add your code here\n","\n","- Replace *air_test* with your google drive location in the sys.path.append()"],"metadata":{"colab_type":"text","id":"fvQMmxs0x_x8"}},{"cell_type":"code","execution_count":null,"source":["from allennlp.common import Params, Tqdm\n","from allennlp.common.util import prepare_environment\n","from allennlp.data.dataloader import PyTorchDataLoader\n","from anyio import current_default_worker_thread_limiter\n","prepare_environment(Params({})) # sets the seeds to be fixed\n","\n","import torch\n","import pandas as pd\n","\n","\n","from allennlp.data.vocabulary import Vocabulary\n","\n","from allennlp.modules.token_embedders import Embedding\n","from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n","\n","from data_loading import *\n","from model_knrm import *\n","from model_conv_knrm import *\n","from model_tk import *\n","\n","#def RR@k\n","def RR(docIds, docScores, docIdsRelevant, k):\n","    #sort docScores in descending order and consider only the top k entries\n","    data = pd.DataFrame({\"doc_ids\":docIds, \"docScores\": docScores})\\\n","        .sort_values(\"docScores\", ascending=False)\\\n","            .reset_index(drop=True)\\\n","                .iloc[0:k]\n","    \n","    #check which of the top k entries is relevant according to docIdsRelevant\n","    check = np.column_stack(\n","        [data.doc_ids == rel for rel in docIdsRelevant]\n","        ).any(axis=1)\n","    \n","    #extract the index of the first relevant document\n","    ind = np.where(check == True)[0]\n","    if len(ind)>0:\n","        return(1/(ind[0]+1))\n","    else:\n","        return(0)\n","\n","# change paths to your data directory\n","config = {\n","    \"vocab_directory\": \"../data/Part-2/allen_vocab_lower_10\",\n","    \"pre_trained_embedding\": \"../data/Part-2/glove.42B.300d.txt\",\n","    \"model\": \"conv_knrm\",\n","    \"train_data\": \"../data/Part-2/triples.train.tsv\",\n","    \"validation_data\": \"../data/Part-2/msmarco_tuples.validation.tsv\",\n","    \"test_data\":\"../data/Part-2/msmarco_tuples.test.tsv\",\n","}\n","\n","\n","#\n","# data loading\n","#\n","\n","vocab = Vocabulary.from_files(config[\"vocab_directory\"])\n","tokens_embedder = Embedding(vocab=vocab,\n","                           pretrained_file= config[\"pre_trained_embedding\"],\n","                           embedding_dim=300,\n","                           trainable=True,\n","                           padding_index=0)\n","word_embedder = BasicTextFieldEmbedder({\"tokens\": tokens_embedder})\n","\n","# recommended default params for the models (but you may change them if you want)\n","if config[\"model\"] == \"knrm\":\n","    model = KNRM(word_embedder, n_kernels=11)\n","elif config[\"model\"] == \"conv_knrm\":\n","    model = Conv_KNRM(word_embedder, n_grams=3, n_kernels=11, conv_out_dim=128)\n","elif config[\"model\"] == \"tk\":\n","    model = TK(word_embedder, n_kernels=11, n_layers = 2, n_tf_dim = 300, n_tf_heads = 10)\n","\n","\n","# todo optimizer, loss \n","\n","print('Model',config[\"model\"],'total parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))\n","print('Network:', model)\n","#use plain margin ranking as loss function, set margin=1 to get formula as given on p. 15 of the slides \"introduction to neural re-ranking\"\n","loss_function = torch.nn.MarginRankingLoss(margin=1)\n","\n","#use Adam  as optimiser\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n","\n","#use GPU if available\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","cpu_device = torch.device('cpu')\n","print(\"Using device\", device)    \n","#model.to(device)\n","\n","#\n","# train\n","#\n","\n","_triple_reader = IrTripleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30)\n","_triple_reader = _triple_reader.read(config[\"train_data\"])\n","_triple_reader.index_with(vocab)\n","loader = PyTorchDataLoader(_triple_reader, batch_size=32)\n","\n","#use validation loader within train loop\n","_tuple_reader = IrLabeledTupleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30)\n","_tuple_reader = _tuple_reader.read(config[\"validation_data\"])\n","_tuple_reader.index_with(vocab)\n","validation_loader = PyTorchDataLoader(_tuple_reader, batch_size=128)\n","\n","qrels = pd.read_csv(\"../data/Part-2/msmarco_qrels.txt\", sep=\"\\t\", header= None, names =[\"query_id\",\"hardcoded-Q0\", \"doc_id\", \"relevance-grade\"])\n","qrels.query_id = qrels.query_id.astype(str)\n","qrels.doc_id = qrels.doc_id.astype(str)\n","\n","\n","#save all train losses in a list\n","train_loss = []\n","validation_metric = []\n","\n","for epoch in range(2):\n","    #set model into train mode\n","    model.train()\n","    for batch in Tqdm.tqdm(loader):\n","        # todo train loop\n","        #set all gradients to 0\n","        optimizer.zero_grad()\n","\n","        query, documentPos, documentNeg = batch[\"query_tokens\"], batch[\"doc_pos_tokens\"], batch[\"doc_neg_tokens\"]\n","        #determine output for positive relevance\n","        outputPos = model(query, documentPos)\n","        outputNeg = model(query, documentNeg)\n","        loss = loss_function(outputPos, outputNeg, torch.ones(outputPos.size(), requires_grad=False)) #set y=1 to state that outputPos should be greater than outputNeg\n","        #calculate the gradients backward the loss function\n","        loss.backward()\n","\n","        #change parameters according to optimiser\n","        optimizer.step()\n","    \n","    print(f\"Epoch: {epoch} Loss: {loss}\")\n","    train_loss.append(loss.detach().cpu().numpy() )\n","    \n","    MRR = 0        \n","    nrValidationQueries = 0 \n","    for batch in Tqdm.tqdm(validation_loader):\n","        # todo validation loop \n","        query_ids, doc_ids, query, document = batch[\"query_id\"],batch[\"doc_id\"],batch[\"query_tokens\"], batch[\"doc_tokens\"]\n","        output = model(query, document).detach().numpy()      \n","        # todo evaluation\n","        # implement MRR@10\n","        distinct_queries = list(set(query_ids))        \n","        nrValidationQueries = nrValidationQueries + len(distinct_queries)\n","        for currentQuery in distinct_queries:\n","            currentRR= RR(\n","                [dID for qID, dID in zip(query_ids, doc_ids) if qID == currentQuery],\n","                output[[ qID == currentQuery for qID in query_ids]],\n","                qrels.loc[qrels.query_id == currentQuery,\"doc_id\"].to_list(),\n","                10\n","            )\n","            print(currentRR)\n","            MRR = MRR +currentRR\n","        \n","    validation_metric.append(MRR / nrValidationQueries)\n","    \n","#\n","# eval (duplicate for validation inside train loop - but rename \"loader\", since\n","# otherwise it will overwrite the original train iterator, which is instantiated outside the loop)\n","#\n","\n","_tuple_reader = IrLabeledTupleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30)\n","_tuple_reader = _tuple_reader.read(config[\"test_data\"])\n","_tuple_reader.index_with(vocab)\n","loader = PyTorchDataLoader(_tuple_reader, batch_size=128)\n","\n","test_metric = []\n","MRR = 0        \n","nrTestQueries = 0 \n","for batch in Tqdm.tqdm(loader):\n","    # todo test loop \n","    query, document = batch[\"query_tokens\"], batch[\"doc_tokens\"]\n","    output = model(query, document).detach().numpy()\n","    # todo evaluation\n","    query_ids, doc_ids, query, document = batch[\"query_id\"],batch[\"doc_id\"],batch[\"query_tokens\"], batch[\"doc_tokens\"]\n","    output = model(query, document).detach().numpy()      \n","    # todo evaluation\n","    # implement MRR@10\n","    distinct_queries = list(set(query_ids))        \n","    nrTestQueries = nrTestQueries + len(distinct_queries)\n","    for currentQuery in distinct_queries:\n","        currentRR= RR(\n","            [dID for qID, dID in zip(query_ids, doc_ids) if qID == currentQuery],\n","            output[[ qID == currentQuery for qID in query_ids]],\n","            qrels.loc[qrels.query_id == currentQuery,\"doc_id\"].to_list(),\n","            10\n","        )\n","        print(currentRR)\n","        MRR = MRR +currentRR\n","\n","test_metric.append(MRR / nrTestQueries)\n"],"outputs":[],"metadata":{"colab":{},"colab_type":"code","id":"Y_IEUP_2-099"}}]}
